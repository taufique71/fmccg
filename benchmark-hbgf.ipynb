{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2248dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import networkx.algorithms.community as nx_comm\n",
    "from networkx.generators.community import LFR_benchmark_graph\n",
    "from networkx.algorithms import bipartite\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.sparse import coo_array\n",
    "from scipy import sparse\n",
    "from cdlib import algorithms\n",
    "from cdlib import evaluation\n",
    "import sklearn\n",
    "from utils import *\n",
    "from distances import *\n",
    "from consensus import *\n",
    "import math\n",
    "import itertools\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e852ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_name = \"hbgf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa05f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hbgf_consensus(P_list):\n",
    "    k = len(P_list)\n",
    "    n = 0\n",
    "    for cluster in P_list[0][\"partition\"]:\n",
    "        n =  n + len(cluster)\n",
    "    label_matrix = np.full((n, k), -1)\n",
    "    for e in range(k):\n",
    "        P = P_list[e]\n",
    "        clust_asn = clust_lst_to_asn(P[\"partition\"], nelem=n)\n",
    "        label_matrix[:,e] = np.array(clust_asn)\n",
    "    cons_asn = CE.cluster_ensembles(np.transpose(label_matrix), solver=\"hbgf\")\n",
    "    cons_lst = clust_asn_to_lst(cons_asn)\n",
    "    P_star = {\"graph\": None, \"partition\": list(cons_lst)}\n",
    "    return P_star"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac0ea4b",
   "metadata": {},
   "source": [
    "# n=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e24da99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('algorithms.label_propagation(G)', 0), ('algorithms.leiden(G)', 1), ('algorithms.significance_communities(G)', 2), ('algorithms.surprise_communities(G)', 3), ('algorithms.greedy_modularity(G)', 4), ('algorithms.paris(G)', 5), ('algorithms.louvain(G,resolution=0.75,randomize=314159)', 6), ('algorithms.louvain(G,resolution=0.75,randomize=2718)', 7), ('algorithms.louvain(G,resolution=1.0,randomize=314159)', 8), ('algorithms.louvain(G,resolution=1.0,randomize=2718)', 9), ('algorithms.louvain(G,resolution=1.25,randomize=314159)', 10), ('algorithms.louvain(G,resolution=1.25,randomize=2718)', 11), ('algorithms.louvain(G,resolution=1.5,randomize=314159)', 12), ('algorithms.louvain(G,resolution=1.5,randomize=2718)', 13), ('algorithms.infomap(G)', 14), ('algorithms.walktrap(G)', 15), ('algorithms.markov_clustering(G,inflation=1.2,pruning_threshold=0.01,convergence_check_frequency=100)', 16), ('algorithms.markov_clustering(G,inflation=1.2,pruning_threshold=0.001,convergence_check_frequency=100)', 17), ('algorithms.markov_clustering(G,inflation=1.5,pruning_threshold=0.01,convergence_check_frequency=100)', 18), ('algorithms.markov_clustering(G,inflation=1.5,pruning_threshold=0.001,convergence_check_frequency=100)', 19), ('algorithms.markov_clustering(G,inflation=2,pruning_threshold=0.01,convergence_check_frequency=100)', 20), ('algorithms.markov_clustering(G,inflation=2,pruning_threshold=0.001,convergence_check_frequency=100)', 21), ('algorithms.markov_clustering(G,inflation=2.5,pruning_threshold=0.01,convergence_check_frequency=100)', 22), ('algorithms.markov_clustering(G,inflation=2.5,pruning_threshold=0.001,convergence_check_frequency=100)', 23), ('algorithms.em(G,k=24)', 24), ('algorithms.em(G,k=42)', 25), ('algorithms.em(G,k=30)', 26), ('algorithms.em(G,k=31)', 27), ('algorithms.sbm_dl(G)', 28), ('algorithms.spinglass(G,spins=24)', 29), ('algorithms.spinglass(G,spins=42)', 30), ('algorithms.spinglass(G,spins=30)', 31), ('algorithms.spinglass(G,spins=31)', 32), ('algorithms.ricci_community(G,alpha=0.3)', 33), ('algorithms.ricci_community(G,alpha=0.5)', 34), ('algorithms.ricci_community(G,alpha=0.6)', 35), ('algorithms.ricci_community(G,alpha=0.75)', 36)]\n"
     ]
    }
   ],
   "source": [
    "n = 200\n",
    "expected_clusters = []\n",
    "for i in range(4):\n",
    "    expected_clusters.append(random.randint(int(n ** (1. / 3)),3*int(n ** (1. / 2))))\n",
    "    \n",
    "alg_params = {\n",
    "    \"label_propagation\": None,\n",
    "    \"leiden\": None,\n",
    "    \"significance_communities\": None,\n",
    "    \"surprise_communities\": None,\n",
    "    \"greedy_modularity\": None,\n",
    "    \"paris\": None,\n",
    "    \"louvain\": {\n",
    "        \"resolution\": [0.75, 1.0, 1.25, 1.5],\n",
    "        \"randomize\": [314159, 2718]\n",
    "    },\n",
    "    \"infomap\": None,\n",
    "    \"walktrap\": None,\n",
    "    \"markov_clustering\": {\n",
    "        \"inflation\": [1.2, 1.5, 2, 2.5],\n",
    "        \"pruning_threshold\": [0.01, 0.001],\n",
    "        \"convergence_check_frequency\": [100]\n",
    "    },\n",
    "    \"em\": {\n",
    "        \"k\": list(expected_clusters)\n",
    "    },\n",
    "    \"sbm_dl\": None,\n",
    "    \"spinglass\": {\n",
    "        \"spins\": list(expected_clusters)\n",
    "    },\n",
    "    \"ricci_community\": {\n",
    "        \"alpha\": [0.3, 0.5, 0.6, 0.75]\n",
    "    }\n",
    "}\n",
    "\n",
    "clustering_enumeration = []\n",
    "count = 0\n",
    "for alg, params in alg_params.items():\n",
    "    param_combinations = []\n",
    "    param_names = []\n",
    "    if params is not None:\n",
    "        iterables = []\n",
    "        param_names = []\n",
    "        for param in params.keys():\n",
    "            iterables.append(list(params[param]))\n",
    "            param_names.append(param)\n",
    "        param_combinations = list(itertools.product(*iterables))\n",
    "    if len(param_combinations) > 0:\n",
    "        for param_combination in param_combinations:\n",
    "            expr = \"algorithms.\"+alg+\"(G\"\n",
    "            for i in range(len(param_names)):\n",
    "                expr = expr + \",\" + param_names[i] + \"=\" + str(param_combination[i])\n",
    "            expr = expr + \")\"\n",
    "            clustering_enumeration.append((expr,count))\n",
    "            count = count + 1      \n",
    "    else:\n",
    "        expr = \"algorithms.\"+alg+\"(G)\"\n",
    "        clustering_enumeration.append((expr,count))\n",
    "        count = count + 1\n",
    "        \n",
    "print(clustering_enumeration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdb1f090",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LFR/n200/LFR_n200_mu01_gamma30_beta11.mtx\n",
      "mu 1 , number of clusters 122\n",
      "Time: 0.40747547149658203\n",
      "LFR/n200/LFR_n200_mu02_gamma30_beta11.mtx\n",
      "mu 2 , number of clusters 133\n",
      "Time: 0.41968536376953125\n",
      "LFR/n200/LFR_n200_mu03_gamma30_beta11.mtx\n",
      "mu 3 , number of clusters 116\n",
      "Time: 0.4672977924346924\n",
      "LFR/n200/LFR_n200_mu04_gamma30_beta11.mtx\n",
      "mu 4 , number of clusters 130\n",
      "Time: 0.5079178810119629\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "n = 200\n",
    "fileprefix = \"LFR/\" + \"n\" + str(n) + \"/\"\n",
    "mus = [1, 2, 3, 4]\n",
    "#mus = [4]\n",
    "gammas = [30]\n",
    "betas = [11]\n",
    "for mu in mus:\n",
    "    for gamma in gammas:\n",
    "        for beta in betas:\n",
    "            P_list = []\n",
    "            fname = \"LFR_n\" + str(n) + \"_mu0\" + str(mu) + \"_gamma\" + str(gamma) + \"_beta\" + str(beta)\n",
    "            graph_file = fileprefix + fname + \".mtx\"\n",
    "            print(graph_file)\n",
    "            G = None\n",
    "            with open(graph_file) as f:\n",
    "                G = nx.from_scipy_sparse_array(spio.mmread(f), create_using=nx.Graph)\n",
    "                coms = None\n",
    "                for k in clustering_enumeration:\n",
    "                    clust_file = fileprefix + fname + \".\" + str(k[1])\n",
    "                    if Path(clust_file).is_file():\n",
    "                        partition = read_clust_lst(clust_file)\n",
    "                        P_list.append({\"graph\": nx.Graph(G), \"partition\": list(partition)})\n",
    "                t1 = time.time()\n",
    "                P_star = hbgf_consensus(P_list)\n",
    "                t2 = time.time()\n",
    "                print(\"mu\", mu, \", number of clusters\", len(P_star[\"partition\"]))\n",
    "                print(\"Time:\", t2-t1)\n",
    "                write_clust_lst(P_star[\"partition\"], fileprefix + fname + \".\" + cons_name)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f837b1",
   "metadata": {},
   "source": [
    "# n=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcc540d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('algorithms.label_propagation(G)', 0), ('algorithms.leiden(G)', 1), ('algorithms.significance_communities(G)', 2), ('algorithms.surprise_communities(G)', 3), ('algorithms.greedy_modularity(G)', 4), ('algorithms.paris(G)', 5), ('algorithms.louvain(G,resolution=0.75,randomize=314159)', 6), ('algorithms.louvain(G,resolution=0.75,randomize=2718)', 7), ('algorithms.louvain(G,resolution=1.0,randomize=314159)', 8), ('algorithms.louvain(G,resolution=1.0,randomize=2718)', 9), ('algorithms.louvain(G,resolution=1.25,randomize=314159)', 10), ('algorithms.louvain(G,resolution=1.25,randomize=2718)', 11), ('algorithms.louvain(G,resolution=1.5,randomize=314159)', 12), ('algorithms.louvain(G,resolution=1.5,randomize=2718)', 13), ('algorithms.infomap(G)', 14), ('algorithms.walktrap(G)', 15), ('algorithms.markov_clustering(G,inflation=1.2,pruning_threshold=0.01,convergence_check_frequency=100)', 16), ('algorithms.markov_clustering(G,inflation=1.2,pruning_threshold=0.001,convergence_check_frequency=100)', 17), ('algorithms.markov_clustering(G,inflation=1.5,pruning_threshold=0.01,convergence_check_frequency=100)', 18), ('algorithms.markov_clustering(G,inflation=1.5,pruning_threshold=0.001,convergence_check_frequency=100)', 19), ('algorithms.markov_clustering(G,inflation=2,pruning_threshold=0.01,convergence_check_frequency=100)', 20), ('algorithms.markov_clustering(G,inflation=2,pruning_threshold=0.001,convergence_check_frequency=100)', 21), ('algorithms.markov_clustering(G,inflation=2.5,pruning_threshold=0.01,convergence_check_frequency=100)', 22), ('algorithms.markov_clustering(G,inflation=2.5,pruning_threshold=0.001,convergence_check_frequency=100)', 23), ('algorithms.em(G,k=83)', 24), ('algorithms.em(G,k=25)', 25), ('algorithms.em(G,k=93)', 26), ('algorithms.em(G,k=16)', 27), ('algorithms.sbm_dl(G)', 28), ('algorithms.spinglass(G,spins=83)', 29), ('algorithms.spinglass(G,spins=25)', 30), ('algorithms.spinglass(G,spins=93)', 31), ('algorithms.spinglass(G,spins=16)', 32), ('algorithms.ricci_community(G,alpha=0.3)', 33), ('algorithms.ricci_community(G,alpha=0.5)', 34), ('algorithms.ricci_community(G,alpha=0.6)', 35), ('algorithms.ricci_community(G,alpha=0.75)', 36)]\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "expected_clusters = []\n",
    "for i in range(4):\n",
    "    expected_clusters.append(random.randint(int(n ** (1. / 3)),3*int(n ** (1. / 2))))\n",
    "    \n",
    "alg_params = {\n",
    "    \"label_propagation\": None,\n",
    "    \"leiden\": None,\n",
    "    \"significance_communities\": None,\n",
    "    \"surprise_communities\": None,\n",
    "    \"greedy_modularity\": None,\n",
    "    \"paris\": None,\n",
    "    \"louvain\": {\n",
    "        \"resolution\": [0.75, 1.0, 1.25, 1.5],\n",
    "        \"randomize\": [314159, 2718]\n",
    "    },\n",
    "    \"infomap\": None,\n",
    "    \"walktrap\": None,\n",
    "    \"markov_clustering\": {\n",
    "        \"inflation\": [1.2, 1.5, 2, 2.5],\n",
    "        \"pruning_threshold\": [0.01, 0.001],\n",
    "        \"convergence_check_frequency\": [100]\n",
    "    },\n",
    "    \"em\": {\n",
    "        \"k\": list(expected_clusters)\n",
    "    },\n",
    "    \"sbm_dl\": None,\n",
    "    \"spinglass\": {\n",
    "        \"spins\": list(expected_clusters)\n",
    "    },\n",
    "    \"ricci_community\": {\n",
    "        \"alpha\": [0.3, 0.5, 0.6, 0.75]\n",
    "    }\n",
    "}\n",
    "\n",
    "clustering_enumeration = []\n",
    "count = 0\n",
    "for alg, params in alg_params.items():\n",
    "    param_combinations = []\n",
    "    param_names = []\n",
    "    if params is not None:\n",
    "        iterables = []\n",
    "        param_names = []\n",
    "        for param in params.keys():\n",
    "            iterables.append(list(params[param]))\n",
    "            param_names.append(param)\n",
    "        param_combinations = list(itertools.product(*iterables))\n",
    "    if len(param_combinations) > 0:\n",
    "        for param_combination in param_combinations:\n",
    "            expr = \"algorithms.\"+alg+\"(G\"\n",
    "            for i in range(len(param_names)):\n",
    "                expr = expr + \",\" + param_names[i] + \"=\" + str(param_combination[i])\n",
    "            expr = expr + \")\"\n",
    "            clustering_enumeration.append((expr,count))\n",
    "            count = count + 1      \n",
    "    else:\n",
    "        expr = \"algorithms.\"+alg+\"(G)\"\n",
    "        clustering_enumeration.append((expr,count))\n",
    "        count = count + 1\n",
    "        \n",
    "print(clustering_enumeration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72ff39e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LFR/n1000/LFR_n1000_mu01_gamma30_beta11.mtx\n",
      "mu 1 , number of clusters 748\n",
      "Time: 0.8461880683898926\n",
      "LFR/n1000/LFR_n1000_mu02_gamma30_beta11.mtx\n",
      "mu 2 , number of clusters 765\n",
      "Time: 0.8739335536956787\n",
      "LFR/n1000/LFR_n1000_mu03_gamma30_beta11.mtx\n",
      "mu 3 , number of clusters 580\n",
      "Time: 1.1762597560882568\n",
      "LFR/n1000/LFR_n1000_mu04_gamma30_beta11.mtx\n",
      "mu 4 , number of clusters 635\n",
      "Time: 1.426685094833374\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "n = 1000\n",
    "fileprefix = \"LFR/\" + \"n\" + str(n) + \"/\"\n",
    "mus = [1, 2, 3, 4]\n",
    "#mus = [4]\n",
    "gammas = [30]\n",
    "betas = [11]\n",
    "for mu in mus:\n",
    "    for gamma in gammas:\n",
    "        for beta in betas:\n",
    "            P_list = []\n",
    "            fname = \"LFR_n\" + str(n) + \"_mu0\" + str(mu) + \"_gamma\" + str(gamma) + \"_beta\" + str(beta)\n",
    "            graph_file = fileprefix + fname + \".mtx\"\n",
    "            print(graph_file)\n",
    "            G = None\n",
    "            with open(graph_file) as f:\n",
    "                G = nx.from_scipy_sparse_array(spio.mmread(f), create_using=nx.Graph)\n",
    "                coms = None\n",
    "                for k in clustering_enumeration:\n",
    "                    clust_file = fileprefix + fname + \".\" + str(k[1])\n",
    "                    if Path(clust_file).is_file():\n",
    "                        partition = read_clust_lst(clust_file)\n",
    "                        P_list.append({\"graph\": nx.Graph(G), \"partition\": list(partition)})\n",
    "                t1 = time.time()\n",
    "                P_star = hbgf_consensus(P_list)\n",
    "                t2 = time.time()\n",
    "                print(\"mu\", mu, \", number of clusters\", len(P_star[\"partition\"]))\n",
    "                print(\"Time:\", t2-t1)\n",
    "                write_clust_lst(P_star[\"partition\"], fileprefix + fname + \".\" + cons_name)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f4092b",
   "metadata": {},
   "source": [
    "# n=5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "257583a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('algorithms.label_propagation(G)', 0), ('algorithms.leiden(G)', 1), ('algorithms.significance_communities(G)', 2), ('algorithms.surprise_communities(G)', 3), ('algorithms.greedy_modularity(G)', 4), ('algorithms.paris(G)', 5), ('algorithms.louvain(G,resolution=0.75,randomize=314159)', 6), ('algorithms.louvain(G,resolution=0.75,randomize=2718)', 7), ('algorithms.louvain(G,resolution=1.0,randomize=314159)', 8), ('algorithms.louvain(G,resolution=1.0,randomize=2718)', 9), ('algorithms.louvain(G,resolution=1.25,randomize=314159)', 10), ('algorithms.louvain(G,resolution=1.25,randomize=2718)', 11), ('algorithms.louvain(G,resolution=1.5,randomize=314159)', 12), ('algorithms.louvain(G,resolution=1.5,randomize=2718)', 13), ('algorithms.infomap(G)', 14), ('algorithms.walktrap(G)', 15), ('algorithms.markov_clustering(G,inflation=1.2,pruning_threshold=0.01,convergence_check_frequency=100)', 16), ('algorithms.markov_clustering(G,inflation=1.2,pruning_threshold=0.001,convergence_check_frequency=100)', 17), ('algorithms.markov_clustering(G,inflation=1.5,pruning_threshold=0.01,convergence_check_frequency=100)', 18), ('algorithms.markov_clustering(G,inflation=1.5,pruning_threshold=0.001,convergence_check_frequency=100)', 19), ('algorithms.markov_clustering(G,inflation=2,pruning_threshold=0.01,convergence_check_frequency=100)', 20), ('algorithms.markov_clustering(G,inflation=2,pruning_threshold=0.001,convergence_check_frequency=100)', 21), ('algorithms.markov_clustering(G,inflation=2.5,pruning_threshold=0.01,convergence_check_frequency=100)', 22), ('algorithms.markov_clustering(G,inflation=2.5,pruning_threshold=0.001,convergence_check_frequency=100)', 23), ('algorithms.em(G,k=179)', 24), ('algorithms.em(G,k=90)', 25), ('algorithms.em(G,k=53)', 26), ('algorithms.em(G,k=47)', 27), ('algorithms.sbm_dl(G)', 28), ('algorithms.spinglass(G,spins=179)', 29), ('algorithms.spinglass(G,spins=90)', 30), ('algorithms.spinglass(G,spins=53)', 31), ('algorithms.spinglass(G,spins=47)', 32), ('algorithms.ricci_community(G,alpha=0.3)', 33), ('algorithms.ricci_community(G,alpha=0.5)', 34), ('algorithms.ricci_community(G,alpha=0.6)', 35), ('algorithms.ricci_community(G,alpha=0.75)', 36)]\n"
     ]
    }
   ],
   "source": [
    "n = 5000\n",
    "expected_clusters = []\n",
    "for i in range(4):\n",
    "    expected_clusters.append(random.randint(int(n ** (1. / 3)),3*int(n ** (1. / 2))))\n",
    "    \n",
    "alg_params = {\n",
    "    \"label_propagation\": None,\n",
    "    \"leiden\": None,\n",
    "    \"significance_communities\": None,\n",
    "    \"surprise_communities\": None,\n",
    "    \"greedy_modularity\": None,\n",
    "    \"paris\": None,\n",
    "    \"louvain\": {\n",
    "        \"resolution\": [0.75, 1.0, 1.25, 1.5],\n",
    "        \"randomize\": [314159, 2718]\n",
    "    },\n",
    "    \"infomap\": None,\n",
    "    \"walktrap\": None,\n",
    "    \"markov_clustering\": {\n",
    "        \"inflation\": [1.2, 1.5, 2, 2.5],\n",
    "        \"pruning_threshold\": [0.01, 0.001],\n",
    "        \"convergence_check_frequency\": [100]\n",
    "    },\n",
    "    \"em\": {\n",
    "        \"k\": list(expected_clusters)\n",
    "    },\n",
    "    \"sbm_dl\": None,\n",
    "    \"spinglass\": {\n",
    "        \"spins\": list(expected_clusters)\n",
    "    },\n",
    "    \"ricci_community\": {\n",
    "        \"alpha\": [0.3, 0.5, 0.6, 0.75]\n",
    "    }\n",
    "}\n",
    "\n",
    "clustering_enumeration = []\n",
    "count = 0\n",
    "for alg, params in alg_params.items():\n",
    "    param_combinations = []\n",
    "    param_names = []\n",
    "    if params is not None:\n",
    "        iterables = []\n",
    "        param_names = []\n",
    "        for param in params.keys():\n",
    "            iterables.append(list(params[param]))\n",
    "            param_names.append(param)\n",
    "        param_combinations = list(itertools.product(*iterables))\n",
    "    if len(param_combinations) > 0:\n",
    "        for param_combination in param_combinations:\n",
    "            expr = \"algorithms.\"+alg+\"(G\"\n",
    "            for i in range(len(param_names)):\n",
    "                expr = expr + \",\" + param_names[i] + \"=\" + str(param_combination[i])\n",
    "            expr = expr + \")\"\n",
    "            clustering_enumeration.append((expr,count))\n",
    "            count = count + 1      \n",
    "    else:\n",
    "        expr = \"algorithms.\"+alg+\"(G)\"\n",
    "        clustering_enumeration.append((expr,count))\n",
    "        count = count + 1\n",
    "        \n",
    "print(clustering_enumeration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78570767",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LFR/n5000/LFR_n5000_mu01_gamma30_beta11.mtx\n",
      "mu 1 , number of clusters 3451\n",
      "Time: 4.917860984802246\n",
      "LFR/n5000/LFR_n5000_mu02_gamma30_beta11.mtx\n",
      "mu 2 , number of clusters 3762\n",
      "Time: 4.905524730682373\n",
      "LFR/n5000/LFR_n5000_mu03_gamma30_beta11.mtx\n",
      "mu 3 , number of clusters 2953\n",
      "Time: 6.563857078552246\n",
      "LFR/n5000/LFR_n5000_mu04_gamma30_beta11.mtx\n",
      "mu 4 , number of clusters 3499\n",
      "Time: 8.91361403465271\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "n = 5000\n",
    "fileprefix = \"LFR/\" + \"n\" + str(n) + \"/\"\n",
    "mus = [1, 2, 3, 4]\n",
    "#mus = [4]\n",
    "gammas = [30]\n",
    "betas = [11]\n",
    "for mu in mus:\n",
    "    for gamma in gammas:\n",
    "        for beta in betas:\n",
    "            P_list = []\n",
    "            fname = \"LFR_n\" + str(n) + \"_mu0\" + str(mu) + \"_gamma\" + str(gamma) + \"_beta\" + str(beta)\n",
    "            graph_file = fileprefix + fname + \".mtx\"\n",
    "            print(graph_file)\n",
    "            G = None\n",
    "            with open(graph_file) as f:\n",
    "                G = nx.from_scipy_sparse_array(spio.mmread(f), create_using=nx.Graph)\n",
    "                coms = None\n",
    "                for k in clustering_enumeration:\n",
    "                    clust_file = fileprefix + fname + \".\" + str(k[1])\n",
    "                    if Path(clust_file).is_file():\n",
    "                        partition = read_clust_lst(clust_file)\n",
    "                        P_list.append({\"graph\": nx.Graph(G), \"partition\": list(partition)})\n",
    "                t1 = time.time()\n",
    "                P_star = hbgf_consensus(P_list)\n",
    "                t2 = time.time()\n",
    "                print(\"mu\", mu, \", number of clusters\", len(P_star[\"partition\"]))\n",
    "                print(\"Time:\", t2-t1)\n",
    "                write_clust_lst(P_star[\"partition\"], fileprefix + fname + \".\" + cons_name)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d59ff26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
