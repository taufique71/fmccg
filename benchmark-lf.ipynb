{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08df444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import networkx.algorithms.community as nx_comm\n",
    "from networkx.generators.community import LFR_benchmark_graph\n",
    "from networkx.algorithms import bipartite\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.sparse import coo_array\n",
    "from scipy import sparse\n",
    "from cdlib import algorithms\n",
    "from cdlib import evaluation\n",
    "import sklearn\n",
    "from utils import *\n",
    "from distances import *\n",
    "from consensus import *\n",
    "import math\n",
    "import itertools\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffaa993",
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_name = \"lf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac0ea4b",
   "metadata": {},
   "source": [
    "## Parameter configurations for clustering generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e24da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "expected_clusters = []\n",
    "for i in range(4):\n",
    "    expected_clusters.append(random.randint(int(n ** (1. / 3)),3*int(n ** (1. / 2))))\n",
    "    \n",
    "alg_params = {\n",
    "    \"label_propagation\": None,\n",
    "    \"leiden\": None,\n",
    "    \"significance_communities\": None,\n",
    "    \"surprise_communities\": None,\n",
    "    \"greedy_modularity\": None,\n",
    "    \"paris\": None,\n",
    "    \"louvain\": {\n",
    "        \"resolution\": [0.75, 1.0, 1.25, 1.5],\n",
    "        \"randomize\": [314159, 2718]\n",
    "    },\n",
    "    \"infomap\": None,\n",
    "    \"walktrap\": None,\n",
    "    \"markov_clustering\": {\n",
    "        \"inflation\": [1.2, 1.5, 2, 2.5],\n",
    "        \"pruning_threshold\": [0.01, 0.001],\n",
    "        \"convergence_check_frequency\": [100]\n",
    "    },\n",
    "    \"em\": {\n",
    "        \"k\": list(expected_clusters)\n",
    "    },\n",
    "    \"sbm_dl\": None,\n",
    "    \"spinglass\": {\n",
    "        \"spins\": list(expected_clusters)\n",
    "    },\n",
    "    \"ricci_community\": {\n",
    "        \"alpha\": [0.3, 0.5, 0.6, 0.75]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8558fa",
   "metadata": {},
   "source": [
    "## Enumerate clusterings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b22489",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_enumeration = []\n",
    "count = 0\n",
    "for alg, params in alg_params.items():\n",
    "    param_combinations = []\n",
    "    param_names = []\n",
    "    if params is not None:\n",
    "        iterables = []\n",
    "        param_names = []\n",
    "        for param in params.keys():\n",
    "            iterables.append(list(params[param]))\n",
    "            param_names.append(param)\n",
    "        param_combinations = list(itertools.product(*iterables))\n",
    "    if len(param_combinations) > 0:\n",
    "        for param_combination in param_combinations:\n",
    "            expr = \"algorithms.\"+alg+\"(G\"\n",
    "            for i in range(len(param_names)):\n",
    "                expr = expr + \",\" + param_names[i] + \"=\" + str(param_combination[i])\n",
    "            expr = expr + \")\"\n",
    "            clustering_enumeration.append((expr,count))\n",
    "            count = count + 1      \n",
    "    else:\n",
    "        expr = \"algorithms.\"+alg+\"(G)\"\n",
    "        clustering_enumeration.append((expr,count))\n",
    "        count = count + 1\n",
    "print(clustering_enumeration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56040b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lancichinetti, A., & Fortunato, S.\n",
    "# Consensus clustering in complex networks. \n",
    "# Scientific reports, 2(1), 1-7. (2012)\n",
    "def lf_consensus(P_list):\n",
    "    G = nx.Graph(P_list[0][\"graph\"])\n",
    "    n = len(list(G.nodes()))\n",
    "    k = len(P_list)\n",
    "    #print(\"Number of nodes\", n)\n",
    "    \n",
    "    row = []\n",
    "    col = []\n",
    "    val = []\n",
    "    for x in P_list:\n",
    "        graph = x[\"graph\"]\n",
    "        partition = x[\"partition\"]\n",
    "        for cluster in partition:\n",
    "            for i in range(len(cluster)):\n",
    "                for j in range(i+1, len(cluster)):\n",
    "                    item_1 = cluster[i]\n",
    "                    item_2 = cluster[j]\n",
    "                    row.append(int(item_1))\n",
    "                    col.append(int(item_2))\n",
    "                    val.append(int(1))\n",
    "                    \n",
    "    r = coo_array((val, (row, col)), shape=(n, n))\n",
    "    \n",
    "    Ga = nx.from_scipy_sparse_array(r)\n",
    "    \n",
    "    #filter_mat = nx.to_numpy_array(P_list[0][\"graph\"])\n",
    "    #nz_rows, nz_cols = np.nonzero(filter_mat)\n",
    "    #adj_mat_target = np.zeros((n,n))\n",
    "    #for i in range(nz_rows.shape[0]):\n",
    "        #adj_mat_target[nz_rows[i], nz_cols[i]] = adj_mat[nz_rows[i], nz_cols[i]] # Need to figure out better way to do it\n",
    "    #Ga = nx.from_numpy_matrix(adj_mat_target)\n",
    "    clust_lst = nx_comm.louvain_communities(Ga, weight=\"weight\", seed=123)\n",
    "    P_star = { \"graph\": nx.Graph(Ga), \"partition\": list(clust_lst)}\n",
    "    return P_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ae654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import networkx as nx\n",
    "import networkx.algorithms.community as nx_comm\n",
    "from networkx.generators.community import LFR_benchmark_graph\n",
    "from networkx.algorithms import bipartite\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import sparse\n",
    "\n",
    "algs = [\"infomap\", \"louvain\", \"leiden\", \"cnm\", \"label-prop\", \"markov\", \"walktrap\", \"spinglass\"]\n",
    "n = 1000\n",
    "fileprefix = \"LFR/\" + \"n\" + str(n) + \"/\"\n",
    "mus = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "#mus = [2]\n",
    "gammas = [30]\n",
    "betas = [11]\n",
    "for mu in mus:\n",
    "    for gamma in gammas:\n",
    "        for beta in betas:\n",
    "            P_list = []\n",
    "            fname = \"LFR_n\" + str(n) + \"_mu0\" + str(mu) + \"_gamma\" + str(gamma) + \"_beta\" + str(beta)\n",
    "            graph_file = fileprefix + fname + \".mtx\"\n",
    "            print(graph_file)\n",
    "            G = None\n",
    "            \n",
    "            with open(graph_file) as f:\n",
    "                G = nx.from_scipy_sparse_matrix(spio.mmread(f), create_using=nx.Graph)\n",
    "                for alg in algs:\n",
    "                    clust_file = fileprefix + fname + \".\" + alg\n",
    "                    partition = read_clust_lst(clust_file)\n",
    "                    P_list.append({\"graph\": nx.Graph(G), \"partition\": list(partition)})\n",
    "                P_star_lf = lf_consensus(P_list)\n",
    "                write_clust_lst(P_star_lf[\"partition\"], fileprefix + fname + \".lf\")\n",
    "                print(\"LF:\", len(P_star_lf[\"partition\"]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8134bf47",
   "metadata": {},
   "source": [
    "# Iterative LF runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e306c8b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "fileprefix = \"LFR/\" + \"n\" + str(n) + \"/\"\n",
    "mus = [1, 2, 3, 4]\n",
    "#mus = [1]\n",
    "gammas = [30]\n",
    "betas = [11]\n",
    "\n",
    "stats = []\n",
    "\n",
    "for mu in mus:\n",
    "    for gamma in gammas:\n",
    "        for beta in betas:\n",
    "            P_list = []\n",
    "            fname = \"LFR_n\" + str(n) + \"_mu0\" + str(mu) + \"_gamma\" + str(gamma) + \"_beta\" + str(beta)\n",
    "            graph_file = fileprefix + fname + \".mtx\"\n",
    "            print(graph_file)\n",
    "            G = None\n",
    "            with open(graph_file) as f:\n",
    "                G = nx.from_scipy_sparse_array(spio.mmread(f), create_using=nx.Graph)\n",
    "                new_adj_mat = nx.to_numpy_array(G)\n",
    "                old_adj_mat = np.zeros(new_adj_mat.shape)\n",
    "                diff_mat = old_adj_mat - new_adj_mat\n",
    "                old_adj_mat = np.array(new_adj_mat)\n",
    "                norm = np.linalg.norm(diff_mat)\n",
    "                P_star = None\n",
    "                for it in range(20):\n",
    "                    P_list = []\n",
    "                    if it > 0:\n",
    "                        for k in clustering_enumeration:\n",
    "                            try:\n",
    "                                coms = eval(k[0])\n",
    "                                print(\"mu:\", mu, \"it:\", it, k[0], len(coms.communities))\n",
    "                                P_list.append({\"graph\": nx.Graph(G), \"partition\": list(coms.communities)})\n",
    "                                stats.append({\"mu\": mu, \"it\": it, \"norm\": norm, \"alg\": k[0], \"ncluster\": len(coms.communities)})\n",
    "                                count = count + 1\n",
    "                            except:\n",
    "                                print(\"UNSUCCESSFUL\", expr)\n",
    "                    else:\n",
    "                        for k in clustering_enumeration:\n",
    "                            clust_file = fileprefix + fname + \".\" + str(k[1])\n",
    "                            if Path(clust_file).is_file():\n",
    "                                partition = read_clust_lst(clust_file)\n",
    "                                print(\"mu:\", mu, \"it:\", it, k[0], len(partition))\n",
    "                                P_list.append({\"graph\": nx.Graph(G), \"partition\": list(partition)})\n",
    "                                stats.append({\"mu\": mu, \"it\": it, \"norm\": norm, \"alg\": k[0], \"ncluster\": len(partition)})\n",
    "                    P_star = lf_consensus(P_list)\n",
    "                    new_adj_mat = nx.to_numpy_array(P_star[\"graph\"])\n",
    "                    G = nx.Graph(P_star[\"graph\"])\n",
    "                    diff_mat = old_adj_mat - new_adj_mat\n",
    "                    norm = np.linalg.norm(diff_mat)\n",
    "                    old_adj_mat = np.array(new_adj_mat)\n",
    "                    stats.append({\"mu\": mu, \"it\": it, \"norm\": norm, \"alg\": \"lf\", \"ncluster\": len(P_star[\"partition\"])})\n",
    "                    print(\"mu:\", mu, \"it:\", it, \"norm:\", norm)\n",
    "                    \n",
    "                    write_clust_lst(P_star[\"partition\"], fileprefix + fname + \".\" + cons_name)\n",
    "        \n",
    "df = pd.DataFrame(stats)\n",
    "df.to_csv(\"benchmark-lf-convergence-multi-alg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e6a48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
